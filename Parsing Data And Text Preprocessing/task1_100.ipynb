{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "# FIT5196 Task 1 in Assessment 1\n",
    "#### Student Name 1: Animesh Dubey\n",
    "#### Student ID: 33758484\n",
    "#### Student Name 2: Ashwin Gururaj\n",
    "#### Student ID: 33921199\n",
    "\n",
    "Date: 01 Aug 2024\n",
    "\n",
    "\n",
    "Environment: Python 3.11.5\n",
    "\n",
    "Libraries used:\n",
    "* re\n",
    "* pandas\n",
    "* numpy\n",
    "* json\n",
    "* os\n",
    "* datetime\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries: 're' for regex, 'pandas' for data manipulation, 'numpy' for numerical operations, 'json' for JSON handling, 'os' for OS-level operations, and 'datetime' for date and time management\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'re': This library allows us to use regular expressions, which are patterns that help us search and manipulate strings based on specific criteria. It's particularly useful for extracting information from semi-structured text data.<br>\n",
    "'pandas': A powerful data manipulation library that provides data structures like DataFrames, which make it easier to handle and analyze structured data such as tables.<br>\n",
    "'numpy': This library supports operations on large arrays and matrices, providing a wide array of mathematical functions that are crucial for numerical computations.<br>\n",
    "'json': This module is used for parsing JSON (JavaScript Object Notation) data, which is a common format for exchanging data between a server and a client.<br>\n",
    "'os': The os module helps us interact with the operating system, allowing us to manage file paths and directories, making our code more flexible and adaptable to different environments.<br>\n",
    "'datetime': This module allows us to work with dates and times, which will be important when we need to handle and manipulate timestamp data in our analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the directory of the current script\n",
    "script_dir = os.getcwd()\n",
    "\n",
    "# Define file paths dynamically using os.path.join\n",
    "excel_file_path = os.path.join(script_dir, 'group100.xlsx')\n",
    "input_file_paths = [os.path.join(script_dir, f'group100_{i}.txt') for i in range(15)]\n",
    "csv_output_path = os.path.join(script_dir, 'task1_100.csv')\n",
    "json_output_path = os.path.join(script_dir, 'task1_100.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script_dir variable is set using os.getcwd(), which dynamically retrieves the directory path where the current script is located. This makes the code more portable, allowing it to correctly reference files relative to the script’s location regardless of where the script is executed.<br>\n",
    "File paths (excel_file_path, input_file_paths, csv_output_path, json_output_path) are dynamically defined using os.path.join(), ensuring portability across different systems.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Regular Expression Patterns for Extracting Fields from Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define regex patterns for extracting various fields from the text data.\n",
    "regex_patterns = {\n",
    "    'user_id': re.compile(r\"<\\s*(?:UserId|userid|UserId\\.?|user_id|user)>\\s*(\\d+)\\s*<\", re.IGNORECASE),\n",
    "    'name': re.compile(r\"<\\s*(?:Name|username|user_name|user)>\\s*(.*?)\\s*<\", re.IGNORECASE),\n",
    "    'time': re.compile(r\"<\\s*(?:time|date|Date|Time)>\\s*(\\d+)\\s*<\", re.IGNORECASE),\n",
    "    'rating': re.compile(r\"<\\s*(?:rate|rating|Rate|Rating)>\\s*(\\d+)\\s*<\", re.IGNORECASE),\n",
    "    'text': re.compile(r\"<\\s*(?:Text|review|Review|text|Review_text)>\\s*(.*?)\\s*<\", re.IGNORECASE),\n",
    "    'pics': re.compile(r\"<\\s*(?:Pics|pictures|Pictures|Pics)>\\s*(.*?)\\s*<\", re.IGNORECASE),\n",
    "    'response': re.compile(r\"<\\s*(?:Response|resp|Resp|response)>\\s*(.*?)\\s*<\", re.IGNORECASE),\n",
    "    'gmapID': re.compile(r\"<\\s*(?:gmapID|Gmap_id|gmap_id|GmapID|Gmap_id)\\s*>\\s*([\\w:]+(?:-\\d+)?(?:[A-Za-z0-9_-]+)?)\\s*<\", re.IGNORECASE)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dictionary contains regular expressions (regex) that are used to find specific information within text data.<br>\n",
    "\n",
    "user_id: This regex finds different ways of writing \"user ID\" and extracts the number that follows.<br>\n",
    "name: This regex finds different ways of writing \"name\" and extracts the name that follows.<br>\n",
    "time: This regex finds different ways of writing \"time\" and extracts the time information.<br>\n",
    "rating: This regex finds different ways of writing \"rating\" and extracts the number that follows.<br>\n",
    "text: This regex finds the main text content of the review.<br>\n",
    "pics: This regex finds information related to pictures or images.<br>\n",
    "response: This regex finds different ways of writing \"response\" and extracts the response information.<br>\n",
    "gmapID: This regex finds Google Map IDs in different formats.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to clean up extracted data.\n",
    "def data_cleaning(data):\n",
    "    \"\"\"\n",
    "    Description: Function removes the leading and trailing extra whitespaces to clean the input string\n",
    "    Arguments: Data(str) on which cleaning operation needs to be done\n",
    "    Returns: If data is there then cleaned data(str) or else if data is None then return None\n",
    "    \"\"\"\n",
    "    return data.strip() if data else None\n",
    "\n",
    "# Function to filter emojis from text\n",
    "def remove_emoticons(text):\n",
    "    \"\"\"\n",
    "    Description: Function is used to find and remove the emojis that falls within the defined range of unicodes from the input string\n",
    "    Arguments: text(str), the txt string from which the emojis needs to be removed\n",
    "    Return: str or None, string in which the defined emojis are removed, else None if the text input is None\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        return None\n",
    "    emoticons_pattern = re.compile(\n",
    "        \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        \"]+\",\n",
    "        flags=re.UNICODE,\n",
    "    )\n",
    "    return emoticons_pattern.sub(r'', text)\n",
    "\n",
    "# Function to extract image dimensions from URLs.\n",
    "def parse_dimensions_from_url(url):\n",
    "    \"\"\"\n",
    "    Description: The function reads and extracts the dimensions of the pictures from the url if given in the input files in the format 'w(\\d+)-h(\\d+), url' and\n",
    "                returns the dimensions if the defined pattern is matched\n",
    "    Arguments: url(str), URL or link from which the dimension of the image needs to be extracted\n",
    "    Return: A list of integers containing height and width if the pattern is matched, else an empty list\n",
    "    \"\"\"\n",
    "    match = re.search(r'w(\\d+)-h(\\d+)', url)\n",
    "    if match:\n",
    "        return [int(match.group(1)), int(match.group(2))]\n",
    "    return []\n",
    "\n",
    "# Extract image dimensions from text and handle different formats.\n",
    "def get_image_dimensions(pics_text):\n",
    "    \"\"\"\n",
    "    Description: This function is used to check if the picture dimensions are contained in a list or in dictionary of URLs. If the dimensions are present\n",
    "                in a list format then the dimensions are appended to the defined 'dimensions' empty list, if dimensions are in format of dictionary of URLs\n",
    "                then 'parse_dimensions_from_url()' function is invoked and dimensions are extracted from the URLs\n",
    "    Arguments: pics_text(str), The string literal which contains the picture dimensions\n",
    "    Return: A list of lists typically containing the dimensions of pictures that are height and width, else an empty list if no dimensions are found\n",
    "    \"\"\"\n",
    "    dimensions = []\n",
    "    if pics_text and pics_text != 'None':\n",
    "        try:\n",
    "            pics_data = eval(pics_text)\n",
    "            for pic in pics_data:\n",
    "                if isinstance(pic, list):\n",
    "                    dimensions.append([int(dim) for dim in pic])  # Convert dimensions to integers\n",
    "                elif isinstance(pic, dict) and 'url' in pic:\n",
    "                    for url in pic['url']:\n",
    "                        dim = parse_dimensions_from_url(url)\n",
    "                        if dim:\n",
    "                            dimensions.append(dim)\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing pics_text: {pics_text}, error: {e}\")\n",
    "    return dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Functions:<br>\n",
    "\n",
    "These functions are used to prepare the data for analysis.<br>\n",
    "\n",
    "data_cleaning(): This function removes extra spaces from the beginning and end of text, making sure the data is consistent and clean.<br>\n",
    "remove_emoticons(): This function takes out emojis from text using a special pattern, making the text more standard and preventing unwanted noise in the analysis.<br>\n",
    "parse_dimensions_from_url(): This function gets the width and height of an image from a URL that follows a specific pattern. It returns these numbers as a list.<br>\n",
    "get_image_dimensions(): This function finds and processes the width and height of images from text data. It can handle both lists and dictionaries of URLs. This function combines image dimension data for further analysis.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to preprocess data from input text files and .xlsx file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract relevant data from text content from each file and organize it into a list of dictionaries\n",
    "def retrieve_data_from_txt(content):\n",
    "    \"\"\"\n",
    "    Description: This function iterates through each input file and reads the content present between '<record>' and '</record>' tags treating each one of\n",
    "                them as individual record. It then uses the defined regex pattern to extract the data for the specific fields from each record found.\n",
    "                The data which is extracted is then stored in a dictionary and then further processed into a list of records\n",
    "    Arguments: content(str), content of the input .txt files which contains the records in a semi structured format that need processing and extraction\n",
    "    Return: A list of dictionary, and each dictionary represents data of each single record that has been processed and extracted\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    for match in re.finditer(r\"<record>(.*?)</record>\", content, re.DOTALL):\n",
    "        record_content = match.group(1)\n",
    "        record = {}\n",
    "        for key, pattern in regex_patterns.items():\n",
    "            match_field = pattern.search(record_content)\n",
    "            if key == 'pics':\n",
    "                pic_dims = get_image_dimensions(match_field.group(1)) if match_field else []\n",
    "                record['pic_dims'] = pic_dims\n",
    "            elif key == 'response':\n",
    "                response_value = data_cleaning(match_field.group(1)) if match_field else None\n",
    "                # Only increment response count if response is not None or empty\n",
    "                record['response'] = response_value if response_value else None\n",
    "            else:\n",
    "                record[key] = data_cleaning(match_field.group(1)) if match_field else None\n",
    "        records.append(record)\n",
    "    return records\n",
    "\n",
    "# Function to Clean Excel data by dropping empty rows and irrelevant columns\n",
    "def process_excel_data(df):\n",
    "    \"\"\"\n",
    "    Description: Function is used to perform cleaning operation on .xlsx input file\n",
    "                1. It will drop the rows which consists of NaN values for all the variables in that respective row\n",
    "                2. It wll drop the pre-defined columns which are not necessary for processing and extraction process. Also, if no such pre defined columns are\n",
    "                    found then the errors would be ignored\n",
    "    Arguments: df(Dataframe), dataframe that has been loaded from the input .xlsx data which needs cleaning\n",
    "    Return: df(Dataframe), the cleaned and processed dataframe\n",
    "    \"\"\"\n",
    "    # Drop rows where all elements are NaN\n",
    "    df = df.dropna(how='all')\n",
    "    # Drop unnecessary columns\n",
    "    df = df.drop(columns=['x0', 'x1', 'x2', 'x3', 'x4'], errors='ignore')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'retrieve_data_from_txt()': This function processes semi-structured text data by extracting relevant information based on predefined regex patterns. It iterates through records, captures specific fields like user IDs, ratings, and image dimensions, and compiles this information into a structured format (list of dictionaries).<br>\n",
    "\n",
    "'process_excel_data()': This function cleans the loaded Excel data by removing rows with all missing values and dropping unnecessary columns. This ensures that only relevant and clean data remains for further analysis.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading, Parsing, and Extracting Data from Text Files into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inititlaise a list to hold the extracted records\n",
    "all_records = []\n",
    "\n",
    "# Read and parse each file\n",
    "\"\"\"\n",
    "Description: This code is responsible for reading and parsing each file in input_file_path list. It opean and read each file content and then calls the\n",
    "             retrieve_data_from_txt function to extract the data from the content. Thaty data is stored in all_record list, if this function sucessfully\n",
    "             processes the file then the first and last record of extracted data are printed else no rercords werer extracted are printed.\n",
    "\"\"\"\n",
    "# Iterate through each text file, read the content, extract data, and append it to all_records\n",
    "for file_path in input_file_paths:\n",
    "    if os.path.exists(file_path):  # Check if the file exists before trying to open it\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "            extracted_records = retrieve_data_from_txt(content)\n",
    "            all_records.extend(extracted_records)\n",
    "            if extracted_records:\n",
    "                first_record = extracted_records[0]\n",
    "                last_record = extracted_records[-1]\n",
    "            else:\n",
    "                print(f\"{file_path}: No records extracted\")\n",
    "    else:\n",
    "        print(f\"File does not exist: {file_path}\")\n",
    "\n",
    "# Convert extracted records into a DataFrame for further processing\n",
    "df_txt = pd.DataFrame(all_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of the code looks at each text file in a list of file paths. It checks if the file exists, then uses a function called retrieve_data_from_txt() to get information from the file and puts it in a list called all_records. After looking at all the files, the collected data is turned into a table called df_txt for further analysis. This makes sure that the text data is gathered in a structured way and ready for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert time-related data from numeric format to datetime format\n",
    "df_txt['time'] = pd.to_numeric(df_txt['time'], errors='coerce')\n",
    "df_txt['time'] = pd.to_datetime(df_txt['time'], unit='ms', errors='coerce')\n",
    "\n",
    "# Convert text fields to lowercase and remove any emojis\n",
    "df_txt['text'] = df_txt['text'].str.lower().apply(remove_emoticons)\n",
    "df_txt['response'] = df_txt['response'].str.lower().apply(remove_emoticons)\n",
    "\n",
    "# Load and combine all sheets from the Excel file into a single DataFrame and clean it\n",
    "df_all_sheets_from_excel = pd.read_excel(excel_file_path, sheet_name=None)\n",
    "excel_dataframe = pd.concat(df_all_sheets_from_excel.values(), ignore_index=True)\n",
    "excel_dataframe = process_excel_data(excel_dataframe)\n",
    "\n",
    "# Ensure consistency in column names for merging purposes\n",
    "excel_dataframe = excel_dataframe.rename(columns={'gmap_id': 'gmapID'})\n",
    "\n",
    "# Merge extracted text data and cleaned Excel data into a combined DataFrame\n",
    "df_combined = pd.concat([excel_dataframe, df_txt], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of the code cleans and prepares the data that was extracted:<br>\n",
    "\n",
    "Timestamp Conversion: The code changes the \"time\" information from numbers to a date and time format, which makes it easier to work with dates and times.<br>\n",
    "Text Cleaning: It removes emojis from the \"text\" and \"response\" parts and makes all the letters lowercase, making the text analysis more consistent.<br>\n",
    "Excel Data Loading and Cleaning: The data from the Excel file is loaded, cleaned by removing unwanted columns and empty rows, and then renamed to make sure the column names are the same.<br>\n",
    "Data Merging: Finally, the cleaned Excel data is combined with the data from the text files into a single table called \"df_combined\" for a complete analysis, making sure all the important information is in one place.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating CSV output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by 'gmapID' and calculate aggregated counts for reviews, texts, and responses\n",
    "csv_output = df_combined.groupby('gmapID').agg(\n",
    "    review_count=('user_id', 'count'),\n",
    "    review_text_count=('text', lambda x: x.dropna().apply(lambda y: y if y.lower() != 'none' else None).dropna().ne('').sum()),\n",
    "    response_count=('response', lambda x: x.dropna().apply(lambda y: y if y.strip().lower() != 'none' else None).dropna().ne('').sum())\n",
    ").reset_index()\n",
    "\n",
    "# Save the aggregated data to a CSV output file\n",
    "csv_output.to_csv(csv_output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of the code groups the data by Google Map ID and calculates important numbers:<br>\n",
    "\n",
    "review_count: This counts how many reviews each Google Map ID has.<br>\n",
    "review_text_count: This counts how many reviews are not empty and valid.<br>\n",
    "response_count: This counts how many responses are not empty and valid.<br>\n",
    "The grouped and calculated data is then saved as a CSV file in a specific location, making it ready for further analysis or reporting.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating JSON output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN with None in the DataFrame for JSON export\n",
    "df_combined = df_combined.where(pd.notnull(df_combined), None)\n",
    "\n",
    "# Prepare and write the JSON output file\n",
    "json_output = {}\n",
    "for gmapID, group in df_combined.groupby('gmapID'):\n",
    "    reviews = group.apply(lambda row: {\n",
    "        'user_id': row['user_id'] if pd.notnull(row['user_id']) else 'None',\n",
    "        'time': pd.to_datetime(row['time']).strftime('%Y-%m-%d %H:%M:%S') if pd.notnull(row['time']) else 'None',\n",
    "        'review_rating': row['rating'] if pd.notnull(row['rating']) else 'None',\n",
    "        'review_text': row['text'] if pd.notnull(row['text']) else 'None',\n",
    "        'If_pic': 'Y' if row['pic_dims'] else 'N',\n",
    "        'pic_dim': row['pic_dims'] if row['pic_dims'] else [],\n",
    "        'If_response': 'Y' if pd.notna(row['response']) and str(row['response']).strip().lower() != 'none' else 'N'\n",
    "    }, axis=1).tolist()\n",
    "\n",
    "    json_output[gmapID] = {\n",
    "        'reviews': reviews,\n",
    "        'earliest_review_date': pd.to_datetime(group['time'].min()).strftime('%Y-%m-%d %H:%M:%S') if pd.notnull(group['time'].min()) else None,\n",
    "        'latest_review_date': pd.to_datetime(group['time'].max()).strftime('%Y-%m-%d %H:%M:%S') if pd.notnull(group['time'].max()) else None\n",
    "    }\n",
    "    \n",
    "# Convert the data to a JSON string\n",
    "json_str = json.dumps(json_output, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Custom replacement to format pic_dim specifically\n",
    "json_str = re.sub(r'\\[\\n\\s*(\\d+),\\s*\\n\\s*(\\d+)\\n\\s*\\]', r'[\"\\1\", \"\\2\"]', json_str)\n",
    "\n",
    "# Write the JSON string into a file\n",
    "with open(json_output_path, 'w', encoding='utf-8') as json_file:\n",
    "    json_file.write(json_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of the code creates and saves the data in a JSON format:<br>\n",
    "\n",
    "Handling Missing Values: Any missing values in the data are replaced with \"None\" so they can be used in the JSON format.<br>\n",
    "Preparing JSON Data: For each Google Map ID, the code collects important review information like user ID, time, rating, review text, picture size, and response status. This information is organized into a dictionary. The earliest and latest review dates for each Google Map ID are also found.<br>\n",
    "Formatting and Saving: The organized data is turned into a JSON string, with a special format for picture sizes. This JSON string is then saved to a file, creating a well-structured JSON output that can be easily shared or used for more work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"> \n",
    "\n",
    "## References <a class=\"anchor\" name=\"Ref\"></a>\n",
    "\n",
    "* Python Software Foundation. (n.d.). os.path — Common pathname manipulations. Python Documentation. https://docs.python.org/3/library/os.path.html\n",
    "* Stack Overflow. (2024, February 23). Iteration over rows and apply function based on condition. https://stackoverflow.com/questions/78930375/iteration-over-rows-and-apply-function-based-on-condition\n",
    "\n",
    "## Acknowledgement\n",
    "* We acknowledge the assistance of ChatGPT, powered by OpenAI, in completing certain parts of this assignment. The use of this AI tool provided valuable support in areas such as   regex patterns, text preprocessing, and enhancing the overall quality of the work. \n",
    "OpenAI. (2023). ChatGPT (GPT-4). https://openai.com/chatgpt\n",
    "\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
