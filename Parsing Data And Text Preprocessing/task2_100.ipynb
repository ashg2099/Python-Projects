{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "# FIT5196 Task 2 in Assessment 1\n",
    "#### Student Name 1: Animesh Dubey\n",
    "#### Student ID: 33758484\n",
    "#### Student Name 2: Ashwin Gururaj\n",
    "#### Student ID: 33921199\n",
    "\n",
    "Date: 08 Aug 2024\n",
    "\n",
    "\n",
    "Environment: Python 3.11.5\n",
    "\n",
    "Libraries used:\n",
    "* nltk.corpus\n",
    "* nltk.tokenize\n",
    "* nltk.stem\n",
    "* nltk.collocations\n",
    "* collections.defaultdict\n",
    "* collections.Counter\n",
    "* os\n",
    "* json\n",
    "* pandas\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries and Downloading NLTK Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ashwingururaj/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ashwingururaj/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/ashwingururaj/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords, words\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures\n",
    "from collections import defaultdict, Counter\n",
    "import nltk\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Libraries and Downloading NLTK Data:<br>\n",
    "\n",
    "We begin by getting the libraries we need, like nltk, pandas, os, and others. These libraries help us work with text, handle data, and do things with files. We also download important data from NLTK, like stop words and word lists, to help us understand and process natural language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the directory of the current script\n",
    "script_dir = os.getcwd()\n",
    "\n",
    "# Define file paths dynamically using os.path.join\n",
    "csv_output_path = os.path.join(script_dir, 'task1_100.csv')\n",
    "json_output_path = os.path.join(script_dir, 'task1_100.json')\n",
    "stopwords_file_path = os.path.join(script_dir, 'stopwords_en.txt')\n",
    "vocab_file_path = os.path.join(script_dir, 'group100_vocab.txt')\n",
    "countvec_file_path = os.path.join(script_dir, 'group100_countvec.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining File Paths:<br>\n",
    "\n",
    "The paths for the CSV, JSON, stop words, and output files are set up using a special way called os.path.join. This method makes sure that the code works in different places and can be easily used in different computer setups. It makes the code more flexible and easier to move between different environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Eligible Businesses and Reviews Based on Text Review Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV output from Task 1 to identify eligible businesses with at least 70 text reviews\n",
    "csv_df = pd.read_csv(csv_output_path)\n",
    "\n",
    "# Filter gmapId with review_text_count >= 70\n",
    "eligible_gmapIDs = csv_df[csv_df['review_text_count'] >= 70]['gmapID'].tolist()\n",
    "\n",
    "# Load JSON output from Task 1 containing review data\n",
    "with open(json_output_path, 'r') as json_file:\n",
    "    json_data = json.load(json_file)\n",
    "\n",
    "# Filter JSON data to include only businesses with eligible gmapIDs\n",
    "filtered_json_data = {gmapID: data for gmapID, data in json_data.items() if gmapID in eligible_gmapIDs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading CSV and JSON Data:<br>\n",
    "\n",
    "The code begins by loading the data from a CSV file into a table called a DataFrame. This table helps us organize and work with the data. It then filters out the Google Map IDs that don't have enough reviews, focusing on the ones that have sufficient data for analysis.<br>\n",
    "\n",
    "Next, the code loads the data from a JSON file. This file contains more detailed information about each Google Map ID. The code then filters this data to only include the information about the Google Map IDs that passed the review count filter from the CSV file. This ensures that we are working with a consistent dataset that has enough reviews for meaningful analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Filtered JSON Data to DataFrame and Preprocessing Text Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the filtered JSON data into a DataFrame for further processing\n",
    "selected_reviews = []\n",
    "for gmapID, data in filtered_json_data.items():\n",
    "    for review in data['reviews']:\n",
    "        selected_reviews.append({\n",
    "            'gmapID': gmapID,\n",
    "            'text': review['review_text']\n",
    "        })\n",
    "\n",
    "df_reviews = pd.DataFrame(selected_reviews)\n",
    "\n",
    "# Dropping NaN values and ensuring the text is in lowercase\n",
    "df_reviews.dropna(subset=['text'], inplace=True)\n",
    "df_reviews['text'] = df_reviews['text'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting Filtered JSON Data to DataFrame:\n",
    "\n",
    "The data from the filtered JSON file is transformed into a DataFrame, a structured table that makes it easier to work with and analyze the data. This step is crucial for transforming the semi-structured JSON data into a format that is more suitable for further processing.<br>\n",
    "\n",
    "By converting the JSON data into a DataFrame, we can take advantage of the powerful data manipulation and analysis capabilities offered by the Pandas library. This includes tasks such as filtering, sorting, grouping, and calculating summary statistics, which are essential for extracting valuable insights from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Text Reviews to Retain Only Valid English Words and Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further filter text to keep only valid English words using NLTK's words corpus\n",
    "english_words = set(words.words())\n",
    "\n",
    "# Load the custom stopwords from a provided file and combine with NLTK stopwords\n",
    "with open(stopwords_file_path, 'r') as f:\n",
    "    custom_stopwords = set(f.read().splitlines())\n",
    "\n",
    "# Combine custom stopwords with NLTK's English stopwords\n",
    "user_defined_stopwords = set(stopwords.words('english')).union(custom_stopwords)\n",
    "\n",
    "# Function to filter only valid English words from the text\n",
    "def filter_english_words(text):\n",
    "    \"\"\"\n",
    "    Description: Filters out non-English words from a given text. This function splits the input text into individual tokens (words), \n",
    "                 checks each token against a predefined set of valid English words, and returns a new string containing only valid English words.\n",
    "    Arguments:\n",
    "        text (str): The input text string that needs to be filtered for English words.\n",
    "    Returns:\n",
    "        str: A string containing only valid English words from the input text, separated by spaces.\n",
    "    \"\"\"\n",
    "    tokens = text.split()\n",
    "    return ' '.join([token for token in tokens if token in english_words])\n",
    "\n",
    "df_reviews['text'] = df_reviews['text'].apply(filter_english_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering for English Words and Removing Stopwords:<br>\n",
    "\n",
    "The review texts are cleaned up to only keep the words that are valid English words. This is done using a list of English words from NLTK. We also remove some common words called stop words that don't add much meaning to the text. This cleaning process is important for making the data better and ensuring that only the important and meaningful words are kept.<br>\n",
    "\n",
    "NLTK word list: This is a list of commonly used English words that is provided by the Natural Language Toolkit (NLTK) library. By filtering the review texts to only include words from this list, we can remove any misspelled or non-English words that might introduce noise or errors into our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing: Tokenization, Stopword Removal, and Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer, stemmer and stopwords\n",
    "tokenizer = RegexpTokenizer(r'[a-zA-Z]+')\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to preprocess text: tokenization, stopword removal, and stemming\n",
    "def text_transformation(text):\n",
    "    \"\"\"\n",
    "    Description: Function to preprocess text data by performing the steps like tokenization, stemming, stopwords removal and custom stopwords removal\n",
    "    Arguments: text(str), The input text that needs to be preprocessed\n",
    "    Return: list(str), A list of processed tokens after preprocessing\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    tokens = [token for token in tokens if token not in user_defined_stopwords]\n",
    "    tokens = [stemmer.stem(token) for token in tokens if len(token) >= 3]\n",
    "    return tokens\n",
    "\n",
    "df_reviews['tokens'] = df_reviews['text'].apply(text_transformation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization, Stemming, and Stopwords Removal:<br>\n",
    "\n",
    "The cleaned review texts are broken down into individual words, called tokens. These tokens are then changed to their root forms, which is called stemming. Finally, any common words that don't add much meaning (stop words) are removed. This process breaks down the text into its most important parts, which are necessary for later analysis like counting how often words appear and finding pairs of words (bigrams).<br>\n",
    "\n",
    "Tokenization: The process of breaking down text into individual words or tokens. This is the first step in most natural language processing tasks.<br>\n",
    "\n",
    "Stemming: The process of reducing words to their root or stem form. For example, the words \"running,\" \"runs,\" and \"ran\" could all be stemmed to the root form \"run.\" This helps to reduce the number of unique words in the text and makes it easier to identify related terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Frequency Analysis and Contextual Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the frequency of tokens for each business\n",
    "business_entities = defaultdict(set)\n",
    "for gmapID, tokens in zip(df_reviews['gmapID'], df_reviews['tokens']):\n",
    "    business_entities[gmapID].update(tokens)\n",
    "\n",
    "# Calculate frequency of each token across all businesses\n",
    "token_frequency_distribution = defaultdict(int)\n",
    "for tokens in business_entities.values():\n",
    "    for token in tokens:\n",
    "        token_frequency_distribution[token] += 1\n",
    "\n",
    "# Remove tokens appearing in more than 95% of businesses (context-dependent stopwords)\n",
    "total_businesses = len(business_entities)\n",
    "context_stopwords = {token for token, freq in token_frequency_distribution.items() if freq / total_businesses >= 0.95}\n",
    "df_reviews['tokens'] = df_reviews['tokens'].apply(lambda tokens: [token for token in tokens if token not in context_stopwords])\n",
    "\n",
    "# Remove rare tokens appearing in less than 5% of businesses\n",
    "low_frequency_tokens = {token for token, freq in token_frequency_distribution.items() if freq / total_businesses < 0.05}\n",
    "df_reviews['tokens'] = df_reviews['tokens'].apply(lambda tokens: [token for token in tokens if token not in low_frequency_tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing Token Frequency and Removing Context-Dependent Stopwords:<br>\n",
    "\n",
    "The code counts how often each word appears in different businesses. It then finds the words that appear in a large percentage of businesses, which are called context-dependent stop words. These words, along with words that appear rarely, are removed to make the analysis better. This helps us focus on the words that are most different between businesses.<br>\n",
    "\n",
    "By removing context-dependent stopwords and rare tokens, we can refine the analysis and focus on the terms that are most discriminative. These terms are more likely to be informative for understanding the differences between businesses and identifying patterns or trends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram Identification and Vocabulary Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all tokens into a single list to find meaningful bigrams\n",
    "aggregate_tokens = [token for tokens in df_reviews['tokens'] for token in tokens]\n",
    "\n",
    "# Initialize Bigram finder and use PMI to identify significant bigrams\n",
    "bigram_analysis = BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(aggregate_tokens)\n",
    "finder.apply_freq_filter(5)  # Ignore bigrams that appear less than 5 times\n",
    "bigrams = finder.nbest(bigram_analysis.pmi, 200)\n",
    "\n",
    "# Combine unigrams and bigrams into a single vocabulary and sort alphabetically\n",
    "vocab = set(aggregate_tokens + [' '.join(bigram) for bigram in bigrams])\n",
    "vocab = sorted(vocab)\n",
    "\n",
    "# Save the vocabulary with token indices to a file\n",
    "vocab_dict = {token: index for index, token in enumerate(vocab)}\n",
    "with open(vocab_file_path, 'w') as f:\n",
    "    for token, index in vocab_dict.items():\n",
    "        f.write(f'{token}:{index}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigram Analysis and Vocabulary Creation:<br>\n",
    "\n",
    "The code finds important pairs of words called bigrams using a method called Pointwise Mutual Information (PMI). It then combines these bigrams with single words called unigrams to make a complete vocabulary. This vocabulary is saved to a file, representing the most important words and phrases from the reviews.<br>\n",
    "\n",
    "Bigram analysis: The process of identifying pairs of words (bigrams) that frequently occur together in the text. This can help to uncover semantic relationships and patterns.<br>\n",
    "Pointwise Mutual Information (PMI): A measure of the statistical association between two words. A higher PMI value indicates a stronger association between the words.<br>\n",
    "\n",
    "By combining bigrams with unigrams, we can create a more comprehensive vocabulary that captures both single words and multi-word phrases. This vocabulary can be used for various natural language processing tasks, such as text classification, topic modeling, and information retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vector Generation and Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate count vectors for each review using the vocabulary\n",
    "def generate_token_count_vector(tokens, vocab_dict):\n",
    "    \"\"\" \n",
    "    Description: Creates a count vector string from a list of words using a vocabulary dictionary.\n",
    "    Arguments:\n",
    "    - tokens (list of str): The words you want to count.\n",
    "    - vocab_dict (dict): A dictionary that maps each word to its corresponding index in the vocabulary.\n",
    "    Returns:\n",
    "    - str: A string showing how many times each word appears, formatted as \"index:count\" and separated by commas.\n",
    "    \"\"\"\n",
    "    token_counts = Counter(tokens)\n",
    "    count_vec = [f\"{vocab_dict[token]}:{count}\" for token, count in token_counts.items() if token in vocab_dict]\n",
    "    return ', '.join(count_vec)\n",
    "\n",
    "# Generate count vectors for each review and save them to the countvec.txt file\n",
    "df_reviews['count_vector'] = df_reviews['tokens'].apply(lambda tokens: generate_token_count_vector(tokens, vocab_dict))\n",
    "\n",
    "# Remove reviews with empty count vectors\n",
    "df_reviews = df_reviews[df_reviews['count_vector'].str.strip() != '']\n",
    "\n",
    "# Aggregate the count vectors for each gmapID\n",
    "aggregated_count_vectors = defaultdict(Counter)\n",
    "\n",
    "for gmapID, count_vector in zip(df_reviews['gmapID'], df_reviews['count_vector']):\n",
    "    if count_vector:\n",
    "        for pair in count_vector.split(', '):\n",
    "            index, freq = pair.split(':')\n",
    "            aggregated_count_vectors[gmapID][index] += int(freq)\n",
    "\n",
    "# Save the aggregated count vectors to a file\n",
    "with open(countvec_file_path, 'w') as f:\n",
    "    for gmapID, counter in aggregated_count_vectors.items():\n",
    "        count_vector_str = ', '.join([f\"{index}:{freq}\" for index, freq in sorted(counter.items(), key=lambda x: int(x[0]))])\n",
    "        f.write(f\"{gmapID}, {count_vector_str}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating Count Vectors and Saving to File:<br>\n",
    "\n",
    "The code creates a number for each review that shows how often each word appears in the vocabulary. These numbers are then grouped by GMap ID and saved to a file. This step changes the text data into a structured format that can be used for more analysis, modeling, or machine learning tasks.<br>\n",
    "\n",
    "Count vector: A numerical representation of a document where each element corresponds to the frequency of a term in the document's vocabulary.<br>\n",
    "\n",
    "Aggregation by gmapID: The process of grouping the count vectors for all reviews associated with a particular GMap ID. This allows us to analyze the overall word usage patterns for each business."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"> \n",
    "\n",
    "## References <a class=\"anchor\" name=\"Ref\"></a>\n",
    "\n",
    "* Bird, S., Klein, E., & Loper, E. (n.d.). Natural Language Toolkit. NLTK Project. https://www.nltk.org/\n",
    "* Stack Overflow. (2015, April 14). Python remove stop words from pandas dataframe. https://stackoverflow.com/questions/29523254/python-remove-stop-words-from-pandas-dataframe\n",
    "* Natural Language Toolkit. (n.d.). collocations.doctest. GitHub. https://github.com/nltk/nltk/blob/develop/nltk/test/collocations.doctest\n",
    "\n",
    "## Acknowledgement\n",
    "* We acknowledge the assistance of ChatGPT, powered by OpenAI, in completing certain parts of this assignment. The use of this AI tool provided valuable support in areas such as   regex patterns, text preprocessing, and enhancing the overall quality of the work. \n",
    "OpenAI. (2023). ChatGPT (GPT-4). https://openai.com/chatgpt\n",
    "\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
